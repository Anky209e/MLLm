# Basic concepts for creating LLM's from Scratch.

- Learning about Tokenization 
  - Word based
  - Byte-pair encoding

- Learning about Embeddings
  - A multi-dimensional vector space representing tokens
  - Tokens with similar context are closer
  - Catches and preserves the context
  - positional_embeddings preservers context w.r.t Position of token in Input
